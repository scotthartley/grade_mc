#! /usr/bin/env python3
"""A simple python script that grades MC output from Scantron forms.

Requires two files. (1) The grading key, a matrix with point values for
each answer deliminated by spaces, one row per question. Zeroes must be
included. (2) The raw output from IT. Correctly handles blank answers
("."). The Scantron key must be the first line (for verification),
usually with uniqueID 00000000. Optionally, a scramble file can be
specified to allow multiple forms to be handled. The scramble file is a
simple matrix with columns of matching question numbers. It does have to
include a column for Form 1 if it is being used.

"""
import numpy as np

# Top and bottom cutoffs for analysis of answers, as fraction. IT uses
# 27% for some reason.
ANALYSIS_THRESHOLD = 0.27


def generate_responses_array(answers):
    """Takes the raw answers from the output and returns the student
    solutions as an array with "1" indicating the selection.

    Args:
        answers (string): A string in the format "010123120...".

    Returns: 
        An array of solutions, with "1" indicating the selected answer.
        For example, a row of [0,1,0,0,0] indicates an answer of B. This
        array yields the total score when multiplied by the key and
        summed.

    """
    responses = []
    for qnum in range(len(answers)):
        response = [0 for n in range(5)]
        # Check if blank response, indicated by ".". If not, change
        # list[answer] to 1.
        if answers[qnum] != ".":
            response[int(answers[qnum])] = 1
        responses.append(response)
    return np.array(responses)


def descramble(responses, formnum, scramble):
    """Unscrambles responses depending on their form.

    Args:
        responses (numpy.array): Array of responses generated by the
            generate_responses_array function.

        formnum (int): The form number for the student.

        scramble (numpy.array): The scramble used for the test. Of form
            scramble[0] = [1, 2, 3, ...] for form 1 (note index
            difference), scramble[n] = [2, 1, 3, ...] for form n+1.

    Returns: An array of responses that has been sorted according to the
        scramble array, so that everyone can be graded with the same
        key.

    """
    descrambled_responses = []
    for n in scramble[formnum]:
        descrambled_responses.append(responses[n-1])

    return np.array(descrambled_responses)


def convert_response_to_letter(response):
    """Converts a grade (as a numpy array) to a letter grade string.

    """
    response_list = response.tolist()
    
    if 1 in response_list:
        return chr(response_list.index(1) + 65)
    else:
        return "."


def main(key_file_name, answers_file_name, title="Graded Exam", 
         scramble_file_name=None):
    """Processes raw Scantron output and returns the grades and
    statistics.

    Args:
        key_file_name (string): Name of the file with the key. The key
            is a matrix in the format [ 0 1 0 0 0; 0 0 1 0 0; ... ]
            where the first row would award 1 point for B. Partial
            credit and double answers are fine.

        answers_file_name (string): Name of the file containing the raw
            Scantron output.

        title="Graded Exam" (string): Title to be printed at the top of
            the output.

        scramble_file_name=None (string): Optional filename of file
            containing the scramble. If not included, everyone is graded
            as the same form as laid out in the key. Format is a matrix
            in the format [ 1 2; 2 3; 3 1; ... ] where form 2 would have
            questions 1, 2, 3 from form 1 as numbers 2, 3, 1.

    Returns:
        A string containing formatted output with exam statistics and
        student scores, and an array of individual student responses.

    """

    # Load the key. Result is a numpy array.
    with open(key_file_name) as key_file:
        ans_key = np.loadtxt(key_file)

    num_questions = len(ans_key)

    # Load and process the scramble file, if available. Transposes so
    # that scramble[0] returns an array of questions for form 1,
    # scramble[2] returns array of questions for form 2, etc.
    if scramble_file_name:
        with open(scramble_file_name) as scramble_file:
            scramble = np.loadtxt(scramble_file).transpose()

    # Load the student info. Characters 0-7 in the input file are the
    # student's uniqueID. Character 9 is the form number. Characters
    # 10-? are the recorded answers. Descrambles the responses. For
    # student n, students[n]['name'] is the student's name,
    # students[n]['responses'] is the set of responses, as an array.
    # Only collects the responses portion of the string (up to
    # 10+num_questions), because the Scantron system can append extra
    # characters.
    students = []
    with open(answers_file_name) as answers_file:
        for line in answers_file:
            uniqueid = line[0:8]
            if line[9] == " " or line[9] == ".":
                form_num = 0
            else:
                form_num = int(line[9]) - 1
            responses = generate_responses_array(
                    line[10:10 + num_questions].replace("\n", ""))
            if scramble_file_name:
                responses = descramble(responses, form_num, scramble)
            students.append({'name': uniqueid, 'responses': responses, 
                             'form': form_num})

    num_students = len(students[1:])
    num_students_analysis = round(ANALYSIS_THRESHOLD * num_students)

    # Actually determines score for each student. Multiplies sets of
    # responses by the key, then sums over whole array. Score is stored
    # as students[n]['score']
    for stu_num in range(len(students)):
        students[stu_num]['score'] = (students[stu_num]['responses'] * 
                                      ans_key).sum()

    # Generates a new array, students_sorted_grade, that is just sorted
    # by grades. Does not include key (students[0]).
    students_sorted_grade = sorted(students[1:], 
                                   key=lambda s: s['score'], reverse=True)

    # Determines number of each response, by question, for all students,
    # and for the top and bottom students in the class. Values are given
    # as fractions.
    all_answers_frac = (sum(n['responses'] 
                        for n in students_sorted_grade[:]) / num_students)
    top_answers_frac = (sum(n['responses'] 
                        for n in students_sorted_grade[:num_students_analysis]) 
                        / num_students_analysis)
    bot_answers_frac = (sum(n['responses'] 
                        for n in students_sorted_grade[-num_students_analysis:]) 
                        / num_students_analysis)

    # List of all grades. Students only (key not included).
    all_grades = [n['score'] for n in students[1:]]

    # The score for the Scantron key, as a check to make sure it was
    # assessed properly.
    max_score = students[0]['score']
    print("\nCheck: the Scantron key (uniqueID = {}) scores {:.2f}.\n"
        .format(students[0]['name'], max_score))

    # Variable output_text is the actual textual output of the function.
    output_text = ""

    output_text += "{}\n".format(title)
    output_text += "{}\n\n".format("=" * len(title))

    # The overall averages, max, and min.
    output_text += "   Overall average: {:.2f} "\
                   "out of {:.2f} points ({:.2%})\n".format(
                    np.mean(all_grades), 
                    max_score, 
                    np.mean(all_grades) / max_score)
    output_text += "Standard deviation: {:.2f}\n".format(np.std(all_grades))
    output_text += "              High: {}\n".format(max(all_grades))
    output_text += "               Low: {}\n".format(min(all_grades))
    output_text += "\n"

    # Breakdown by question. Includes both average score for the
    # question, the overall performance, and the performance of the
    # strongest and weakest students.
    output_text += "Average Scores by Question\n"
    output_text += "{}\n\n".format("-" * 26)

    for n in range(num_questions):
        output_text += "{:3}: {:.2f}         Key: ".format(
                       n + 1, sum(all_answers_frac[n] * ans_key[n]))
        for m in range(len(ans_key[n])):
            if ans_key[n][m] != 0:
                output_text += "{:6.1f}  ".format(ans_key[n][m])
            else:
                output_text += "    -   "
        output_text += "\n            Frequency:  "
        for m in range(len(all_answers_frac[n])):
            output_text += "{:5.1f}   ".format(all_answers_frac[n][m] * 100)
        output_text += "(%)\n              Top {:2.0f}%:  ".format(
                       ANALYSIS_THRESHOLD * 100)
        for m in range(len(top_answers_frac[n])):
            output_text += "{:5.1f}   ".format(top_answers_frac[n][m] * 100)
        output_text += "\n              Bot {:2.0f}%:  ".format(
                       ANALYSIS_THRESHOLD * 100)
        for m in range(len(bot_answers_frac[n])):
            output_text += "{:5.1f}   ".format(bot_answers_frac[n][m] * 100)
        output_text += "\n\n"

    # Actual student scores.
    students_sorted_name = sorted(students[1:], key=lambda s: s['name'])
    output_text += "\nStudent Scores\n"
    output_text += "{}\n".format("-" * 14)
    for student in students_sorted_name:
        output_text += "{:8}\t{:.1f}\n".format(student['name'], 
                       student['score'])

    # Generate individual student output.
    student_output_text = {}

    for student in students:
        print(student['name'])
        student_output =  "{}\n".format(student['name'])
        student_output += "Form: {}\n".format(student['form'] + 1)
        student_output += "Question  Response  Score\n"
        student_output += "========  ========  =====\n"

        # Question output stored in a list that can be sorted later.
        # Necessary otherwise descrambling leaves the questions in the
        # wrong order.
        question_output = []
        for n in range(num_questions):
            if 1 in student['responses'][n]:
                question_output.append("   {:2.0f}"\
                    "         {}      {:1.2f}\n".format(
                    scramble[student['form']][n],
                    convert_response_to_letter(student['responses'][n]), 
                    ans_key[n][student['responses'][n].tolist().index(1)]))
            else:
                question_output.append("   {:2.0f}"\
                    "       {}    {:1.2f}\n".format(
                    scramble[student['form']][n], "Blank", 0))

        for q in sorted(question_output):
            student_output += q

        student_output += "\nTotal: {} / {}".format(
                          student['score'], max_score)
        student_output_text[student['name']] = student_output

    return(output_text, student_output_text)


if __name__ == '__main__':
    key_file = input("Filename for key: ")
    raw_file = input("Filename for student responses: ")
    is_scrambled = input("Are multiple forms used (y/n)? ")
    if is_scrambled in ["y", "Y", "yes", "YES", "Yes"]:
        scramble_file = input("Filename for scramble: ")
    else:
        scramble_file = None
    title = input("Title for results: ")
    output_filename = input("Output filename (blank for output to terminal): ")
    output_individual_responses = input("Output individual"\
                                  " responses as uniqueID.txt (y/n)? ")
    output, student_output = main(key_file, raw_file, title, scramble_file)
    if output_filename:
        with open(output_filename, 'w') as output_file:
            output_file.write(output)
    else:
        print(output)

    if output_individual_responses in ["y", "Y", "yes", "YES", "Yes"]:
        for s in sorted(student_output):
            with open("{}_out.txt".format(s), 'w') as output_file:
                output_file.write(student_output[s])
    else:
        for s in sorted(student_output):
            print(student_output[s])